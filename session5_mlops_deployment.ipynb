{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 (2-4h): 배포 연동 & MLOps\n",
    "\n",
    "## 1. 환경 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 설치\n",
    "!pip install -q mlflow\n",
    "!pip install -q fastapi uvicorn\n",
    "!pip install -q onnx onnxruntime\n",
    "!pip install -q tensorrt  # GPU 환경에서만\n",
    "!pip install -q tritonclient[all]  # Triton Inference Server client\n",
    "!pip install -q docker\n",
    "!pip install -q prometheus-client\n",
    "!pip install -q pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# MLOps Libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Model Conversion\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "# API Development\n",
    "from pydantic import BaseModel, Field\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "# Monitoring\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 서빙을 위한 FastAPI 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastAPI 모델 서빙 구현\n",
    "class ModelServingAPI:\n",
    "    \"\"\"모델 서빙 API 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.app = FastAPI(title=\"Manufacturing AI Model API\", version=\"1.0.0\")\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.model_metadata = {}\n",
    "        \n",
    "        # Prometheus 메트릭 정의\n",
    "        self.prediction_counter = Counter(\n",
    "            'model_predictions_total', \n",
    "            'Total number of predictions'\n",
    "        )\n",
    "        self.prediction_latency = Histogram(\n",
    "            'model_prediction_duration_seconds',\n",
    "            'Prediction latency in seconds'\n",
    "        )\n",
    "        self.active_model_gauge = Gauge(\n",
    "            'active_model_version',\n",
    "            'Currently active model version'\n",
    "        )\n",
    "        \n",
    "        self._setup_routes()\n",
    "    \n",
    "    def _setup_routes(self):\n",
    "        \"\"\"API 라우트 설정\"\"\"\n",
    "        \n",
    "        @self.app.get(\"/\")\n",
    "        async def root():\n",
    "            return {\"message\": \"Manufacturing AI Model API\", \"status\": \"running\"}\n",
    "        \n",
    "        @self.app.get(\"/health\")\n",
    "        async def health_check():\n",
    "            return {\n",
    "                \"status\": \"healthy\",\n",
    "                \"model_loaded\": self.model is not None,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        \n",
    "        @self.app.get(\"/model/info\")\n",
    "        async def model_info():\n",
    "            return self.model_metadata\n",
    "        \n",
    "        @self.app.post(\"/predict\")\n",
    "        async def predict(request: PredictionRequest):\n",
    "            return await self._predict(request)\n",
    "        \n",
    "        @self.app.post(\"/batch_predict\")\n",
    "        async def batch_predict(request: BatchPredictionRequest):\n",
    "            return await self._batch_predict(request)\n",
    "        \n",
    "        @self.app.get(\"/metrics\")\n",
    "        async def metrics():\n",
    "            return generate_latest()\n",
    "    \n",
    "    async def _predict(self, request: PredictionRequest):\n",
    "        \"\"\"단일 예측 수행\"\"\"\n",
    "        if self.model is None:\n",
    "            raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 데이터 전처리\n",
    "            features = np.array(request.features).reshape(1, -1)\n",
    "            if self.scaler:\n",
    "                features = self.scaler.transform(features)\n",
    "            \n",
    "            # 예측\n",
    "            prediction = self.model.predict(features)[0]\n",
    "            \n",
    "            # 확률 (분류 모델인 경우)\n",
    "            if hasattr(self.model, 'predict_proba'):\n",
    "                probabilities = self.model.predict_proba(features)[0].tolist()\n",
    "            else:\n",
    "                probabilities = None\n",
    "            \n",
    "            # 메트릭 업데이트\n",
    "            self.prediction_counter.inc()\n",
    "            self.prediction_latency.observe(time.time() - start_time)\n",
    "            \n",
    "            return {\n",
    "                \"prediction\": int(prediction),\n",
    "                \"probabilities\": probabilities,\n",
    "                \"latency_ms\": (time.time() - start_time) * 1000,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    async def _batch_predict(self, request: BatchPredictionRequest):\n",
    "        \"\"\"배치 예측 수행\"\"\"\n",
    "        if self.model is None:\n",
    "            raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 데이터 전처리\n",
    "            features = np.array(request.batch_features)\n",
    "            if self.scaler:\n",
    "                features = self.scaler.transform(features)\n",
    "            \n",
    "            # 예측\n",
    "            predictions = self.model.predict(features).tolist()\n",
    "            \n",
    "            # 확률\n",
    "            if hasattr(self.model, 'predict_proba'):\n",
    "                probabilities = self.model.predict_proba(features).tolist()\n",
    "            else:\n",
    "                probabilities = None\n",
    "            \n",
    "            # 메트릭 업데이트\n",
    "            self.prediction_counter.inc(len(predictions))\n",
    "            self.prediction_latency.observe(time.time() - start_time)\n",
    "            \n",
    "            return {\n",
    "                \"predictions\": predictions,\n",
    "                \"probabilities\": probabilities,\n",
    "                \"batch_size\": len(predictions),\n",
    "                \"latency_ms\": (time.time() - start_time) * 1000,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Pydantic 모델 정의\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[float] = Field(..., description=\"Input features for prediction\")\n",
    "    request_id: Optional[str] = Field(None, description=\"Optional request ID for tracking\")\n",
    "\n",
    "class BatchPredictionRequest(BaseModel):\n",
    "    batch_features: List[List[float]] = Field(..., description=\"Batch of input features\")\n",
    "    request_id: Optional[str] = Field(None, description=\"Optional request ID for tracking\")\n",
    "\n",
    "# API 인스턴스 생성 예제\n",
    "print(\"FastAPI Model Serving Structure Created\")\n",
    "print(\"\\nAPI Endpoints:\")\n",
    "print(\"  GET  /          - API root\")\n",
    "print(\"  GET  /health    - Health check\")\n",
    "print(\"  GET  /model/info - Model metadata\")\n",
    "print(\"  POST /predict   - Single prediction\")\n",
    "print(\"  POST /batch_predict - Batch predictions\")\n",
    "print(\"  GET  /metrics   - Prometheus metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ONNX/TensorRT 변환 및 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOptimizer:\n",
    "    \"\"\"모델 최적화 및 변환 도구\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimization_results = {}\n",
    "        \n",
    "    def pytorch_to_onnx(self, model, input_shape, output_path=\"model.onnx\"):\n",
    "        \"\"\"PyTorch 모델을 ONNX로 변환\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # 더미 입력 생성\n",
    "        dummy_input = torch.randn(*input_shape)\n",
    "        \n",
    "        # ONNX로 변환\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            output_path,\n",
    "            export_params=True,\n",
    "            opset_version=11,\n",
    "            do_constant_folding=True,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            dynamic_axes={\n",
    "                'input': {0: 'batch_size'},\n",
    "                'output': {0: 'batch_size'}\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # 검증\n",
    "        onnx_model = onnx.load(output_path)\n",
    "        onnx.checker.check_model(onnx_model)\n",
    "        \n",
    "        print(f\"Model converted to ONNX: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    def optimize_onnx(self, onnx_path, optimization_level=3):\n",
    "        \"\"\"ONNX Runtime 최적화\"\"\"\n",
    "        # Session options\n",
    "        sess_options = ort.SessionOptions()\n",
    "        sess_options.graph_optimization_level = getattr(\n",
    "            ort.GraphOptimizationLevel, \n",
    "            ['DISABLE', 'BASIC', 'EXTENDED', 'ALL'][optimization_level]\n",
    "        )\n",
    "        \n",
    "        # Provider 선택 (GPU 사용 가능시 CUDA)\n",
    "        providers = ['CPUExecutionProvider']\n",
    "        if torch.cuda.is_available():\n",
    "            providers.insert(0, 'CUDAExecutionProvider')\n",
    "        \n",
    "        # ONNX Runtime 세션 생성\n",
    "        session = ort.InferenceSession(onnx_path, sess_options, providers=providers)\n",
    "        \n",
    "        return session\n",
    "    \n",
    "    def benchmark_inference(self, model_func, input_data, n_iterations=100):\n",
    "        \"\"\"추론 성능 벤치마크\"\"\"\n",
    "        latencies = []\n",
    "        \n",
    "        # Warm-up\n",
    "        for _ in range(10):\n",
    "            _ = model_func(input_data)\n",
    "        \n",
    "        # Benchmark\n",
    "        for _ in range(n_iterations):\n",
    "            start = time.perf_counter()\n",
    "            _ = model_func(input_data)\n",
    "            latencies.append((time.perf_counter() - start) * 1000)  # ms\n",
    "        \n",
    "        results = {\n",
    "            'mean_latency_ms': np.mean(latencies),\n",
    "            'std_latency_ms': np.std(latencies),\n",
    "            'min_latency_ms': np.min(latencies),\n",
    "            'max_latency_ms': np.max(latencies),\n",
    "            'p50_latency_ms': np.percentile(latencies, 50),\n",
    "            'p95_latency_ms': np.percentile(latencies, 95),\n",
    "            'p99_latency_ms': np.percentile(latencies, 99),\n",
    "            'throughput_fps': 1000 / np.mean(latencies)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def quantize_model(self, model, calibration_data=None):\n",
    "        \"\"\"모델 양자화 (INT8)\"\"\"\n",
    "        # PyTorch 동적 양자화 예제\n",
    "        quantized_model = torch.quantization.quantize_dynamic(\n",
    "            model,\n",
    "            {nn.Linear, nn.Conv2d},\n",
    "            dtype=torch.qint8\n",
    "        )\n",
    "        \n",
    "        # 모델 크기 비교\n",
    "        def get_model_size(model):\n",
    "            param_size = 0\n",
    "            buffer_size = 0\n",
    "            \n",
    "            for param in model.parameters():\n",
    "                param_size += param.nelement() * param.element_size()\n",
    "            \n",
    "            for buffer in model.buffers():\n",
    "                buffer_size += buffer.nelement() * buffer.element_size()\n",
    "            \n",
    "            return (param_size + buffer_size) / 1024 / 1024  # MB\n",
    "        \n",
    "        original_size = get_model_size(model)\n",
    "        quantized_size = get_model_size(quantized_model)\n",
    "        \n",
    "        print(f\"Original model size: {original_size:.2f} MB\")\n",
    "        print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "        print(f\"Compression ratio: {original_size/quantized_size:.2f}x\")\n",
    "        \n",
    "        return quantized_model\n",
    "\n",
    "# 간단한 PyTorch 모델 예제\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size=10, hidden_size=64, output_size=3):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 모델 최적화 시연\n",
    "print(\"Model Optimization Demonstration\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# PyTorch 모델 생성\n",
    "model = SimpleNN()\n",
    "optimizer_tool = ModelOptimizer()\n",
    "\n",
    "# 1. ONNX 변환\n",
    "print(\"\\n1. Converting to ONNX...\")\n",
    "onnx_path = optimizer_tool.pytorch_to_onnx(\n",
    "    model, \n",
    "    input_shape=(1, 10),\n",
    "    output_path=\"optimized_model.onnx\"\n",
    ")\n",
    "\n",
    "# 2. 양자화\n",
    "print(\"\\n2. Quantizing model...\")\n",
    "quantized_model = optimizer_tool.quantize_model(model)\n",
    "\n",
    "# 3. 벤치마크 (시뮬레이션)\n",
    "print(\"\\n3. Benchmark Results (Simulated):\")\n",
    "dummy_input = torch.randn(1, 10)\n",
    "\n",
    "# 원본 모델 벤치마크\n",
    "original_results = optimizer_tool.benchmark_inference(\n",
    "    lambda x: model(x),\n",
    "    dummy_input,\n",
    "    n_iterations=100\n",
    ")\n",
    "\n",
    "# 양자화 모델 벤치마크\n",
    "quantized_results = optimizer_tool.benchmark_inference(\n",
    "    lambda x: quantized_model(x),\n",
    "    dummy_input,\n",
    "    n_iterations=100\n",
    ")\n",
    "\n",
    "# 결과 비교\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': original_results,\n",
    "    'Quantized': quantized_results\n",
    "}).T\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df[['mean_latency_ms', 'p95_latency_ms', 'throughput_fps']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MLflow를 활용한 모델 버전 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLOpsManager:\n",
    "    \"\"\"MLOps 관리 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self, experiment_name=\"manufacturing_ai\"):\n",
    "        self.experiment_name = experiment_name\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        self.model_registry = {}\n",
    "        \n",
    "    def log_experiment(self, \n",
    "                      model,\n",
    "                      metrics,\n",
    "                      params,\n",
    "                      artifacts=None,\n",
    "                      model_name=\"model\"):\n",
    "        \"\"\"MLflow 실험 로깅\"\"\"\n",
    "        \n",
    "        with mlflow.start_run() as run:\n",
    "            # 파라미터 로깅\n",
    "            for key, value in params.items():\n",
    "                mlflow.log_param(key, value)\n",
    "            \n",
    "            # 메트릭 로깅\n",
    "            for key, value in metrics.items():\n",
    "                mlflow.log_metric(key, value)\n",
    "            \n",
    "            # 모델 로깅\n",
    "            if isinstance(model, torch.nn.Module):\n",
    "                mlflow.pytorch.log_model(model, model_name)\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, model_name)\n",
    "            \n",
    "            # 아티팩트 로깅\n",
    "            if artifacts:\n",
    "                for artifact_path in artifacts:\n",
    "                    mlflow.log_artifact(artifact_path)\n",
    "            \n",
    "            # 태그 추가\n",
    "            mlflow.set_tag(\"environment\", \"production\")\n",
    "            mlflow.set_tag(\"model_type\", type(model).__name__)\n",
    "            \n",
    "            return run.info.run_id\n",
    "    \n",
    "    def register_model(self, run_id, model_name, stage=\"Staging\"):\n",
    "        \"\"\"모델 레지스트리에 등록\"\"\"\n",
    "        model_uri = f\"runs:/{run_id}/model\"\n",
    "        \n",
    "        # 모델 등록\n",
    "        mv = mlflow.register_model(model_uri, model_name)\n",
    "        \n",
    "        # 스테이지 전환\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=mv.version,\n",
    "            stage=stage\n",
    "        )\n",
    "        \n",
    "        self.model_registry[model_name] = {\n",
    "            'version': mv.version,\n",
    "            'stage': stage,\n",
    "            'run_id': run_id\n",
    "        }\n",
    "        \n",
    "        return mv\n",
    "    \n",
    "    def load_production_model(self, model_name):\n",
    "        \"\"\"프로덕션 모델 로드\"\"\"\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        \n",
    "        # Production 스테이지 모델 로드\n",
    "        model_version = client.get_latest_versions(\n",
    "            model_name, \n",
    "            stages=[\"Production\"]\n",
    "        )[0]\n",
    "        \n",
    "        model_uri = f\"models:/{model_name}/{model_version.version}\"\n",
    "        model = mlflow.pyfunc.load_model(model_uri)\n",
    "        \n",
    "        return model, model_version\n",
    "    \n",
    "    def compare_models(self, run_ids):\n",
    "        \"\"\"모델 성능 비교\"\"\"\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        \n",
    "        comparison_data = []\n",
    "        for run_id in run_ids:\n",
    "            run = client.get_run(run_id)\n",
    "            metrics = run.data.metrics\n",
    "            params = run.data.params\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'run_id': run_id,\n",
    "                'accuracy': metrics.get('accuracy', 0),\n",
    "                'f1_score': metrics.get('f1_score', 0),\n",
    "                'latency_ms': metrics.get('latency_ms', 0),\n",
    "                'model_size_mb': metrics.get('model_size_mb', 0)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(comparison_data)\n",
    "    \n",
    "    def automated_model_promotion(self, \n",
    "                                 model_name,\n",
    "                                 metric_threshold={'accuracy': 0.95}):\n",
    "        \"\"\"자동 모델 프로모션\"\"\"\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        \n",
    "        # Staging 모델 가져오기\n",
    "        staging_models = client.get_latest_versions(\n",
    "            model_name, \n",
    "            stages=[\"Staging\"]\n",
    "        )\n",
    "        \n",
    "        if not staging_models:\n",
    "            print(\"No models in Staging\")\n",
    "            return False\n",
    "        \n",
    "        staging_model = staging_models[0]\n",
    "        run = client.get_run(staging_model.run_id)\n",
    "        \n",
    "        # 메트릭 검증\n",
    "        promote = True\n",
    "        for metric, threshold in metric_threshold.items():\n",
    "            value = run.data.metrics.get(metric, 0)\n",
    "            if value < threshold:\n",
    "                promote = False\n",
    "                print(f\"Metric {metric}={value:.3f} below threshold {threshold}\")\n",
    "        \n",
    "        if promote:\n",
    "            # Production으로 프로모션\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=staging_model.version,\n",
    "                stage=\"Production\",\n",
    "                archive_existing_versions=True\n",
    "            )\n",
    "            print(f\"Model {model_name} v{staging_model.version} promoted to Production\")\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "# MLOps 워크플로우 시뮬레이션\n",
    "print(\"MLOps Workflow Simulation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# MLOps Manager 생성\n",
    "mlops = MLOpsManager(experiment_name=\"manufacturing_quality_prediction\")\n",
    "\n",
    "# 샘플 모델 학습 및 로깅\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 데이터 생성\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=3, \n",
    "                          n_informative=8, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 여러 모델 실험\n",
    "experiments = [\n",
    "    {'n_estimators': 50, 'max_depth': 5},\n",
    "    {'n_estimators': 100, 'max_depth': 10},\n",
    "    {'n_estimators': 200, 'max_depth': 15}\n",
    "]\n",
    "\n",
    "run_ids = []\n",
    "for exp_params in experiments:\n",
    "    # 모델 학습\n",
    "    model = RandomForestClassifier(**exp_params, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 평가\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # MLflow 로깅\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'test_samples': len(X_test)\n",
    "    }\n",
    "    \n",
    "    run_id = mlops.log_experiment(\n",
    "        model=model,\n",
    "        metrics=metrics,\n",
    "        params=exp_params,\n",
    "        model_name=\"rf_model\"\n",
    "    )\n",
    "    \n",
    "    run_ids.append(run_id)\n",
    "    print(f\"Experiment logged: n_estimators={exp_params['n_estimators']}, \"\n",
    "          f\"accuracy={accuracy:.3f}, run_id={run_id[:8]}...\")\n",
    "\n",
    "# 모델 비교\n",
    "print(\"\\nModel Comparison:\")\n",
    "# Note: 실제 MLflow 서버가 없으므로 시뮬레이션\n",
    "comparison_data = []\n",
    "for i, (run_id, exp_params) in enumerate(zip(run_ids, experiments)):\n",
    "    comparison_data.append({\n",
    "        'run_id': run_id[:8] + '...',\n",
    "        'n_estimators': exp_params['n_estimators'],\n",
    "        'max_depth': exp_params['max_depth'],\n",
    "        'accuracy': 0.90 + i*0.02,  # 시뮬레이션\n",
    "        'f1_score': 0.89 + i*0.02   # 시뮬레이션\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 모니터링 및 드리프트 감지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMonitoring:\n",
    "    \"\"\"모델 모니터링 및 드리프트 감지\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reference_data = None\n",
    "        self.monitoring_metrics = []\n",
    "        self.drift_history = []\n",
    "        \n",
    "    def set_reference_data(self, X_reference, y_reference=None):\n",
    "        \"\"\"참조 데이터 설정 (학습 데이터)\"\"\"\n",
    "        self.reference_data = {\n",
    "            'X': X_reference,\n",
    "            'y': y_reference,\n",
    "            'statistics': self._calculate_statistics(X_reference)\n",
    "        }\n",
    "    \n",
    "    def _calculate_statistics(self, X):\n",
    "        \"\"\"데이터 통계 계산\"\"\"\n",
    "        return {\n",
    "            'mean': np.mean(X, axis=0),\n",
    "            'std': np.std(X, axis=0),\n",
    "            'min': np.min(X, axis=0),\n",
    "            'max': np.max(X, axis=0),\n",
    "            'quantiles': np.percentile(X, [25, 50, 75], axis=0)\n",
    "        }\n",
    "    \n",
    "    def detect_data_drift(self, X_current, method='ks_test', threshold=0.05):\n",
    "        \"\"\"데이터 드리프트 감지\"\"\"\n",
    "        drift_results = {}\n",
    "        \n",
    "        for feature_idx in range(X_current.shape[1]):\n",
    "            reference_feature = self.reference_data['X'][:, feature_idx]\n",
    "            current_feature = X_current[:, feature_idx]\n",
    "            \n",
    "            if method == 'ks_test':\n",
    "                # Kolmogorov-Smirnov test\n",
    "                statistic, p_value = stats.ks_2samp(\n",
    "                    reference_feature, \n",
    "                    current_feature\n",
    "                )\n",
    "                drift_detected = p_value < threshold\n",
    "                \n",
    "            elif method == 'psi':\n",
    "                # Population Stability Index\n",
    "                psi_value = self._calculate_psi(\n",
    "                    reference_feature, \n",
    "                    current_feature\n",
    "                )\n",
    "                drift_detected = psi_value > 0.2  # PSI threshold\n",
    "                statistic = psi_value\n",
    "                p_value = None\n",
    "            \n",
    "            drift_results[f'feature_{feature_idx}'] = {\n",
    "                'drift_detected': drift_detected,\n",
    "                'statistic': statistic,\n",
    "                'p_value': p_value\n",
    "            }\n",
    "        \n",
    "        # 전체 드리프트 판정\n",
    "        total_features = len(drift_results)\n",
    "        drifted_features = sum(\n",
    "            1 for v in drift_results.values() if v['drift_detected']\n",
    "        )\n",
    "        \n",
    "        overall_drift = drifted_features / total_features > 0.3\n",
    "        \n",
    "        drift_summary = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'overall_drift': overall_drift,\n",
    "            'drifted_features': drifted_features,\n",
    "            'total_features': total_features,\n",
    "            'drift_ratio': drifted_features / total_features,\n",
    "            'details': drift_results\n",
    "        }\n",
    "        \n",
    "        self.drift_history.append(drift_summary)\n",
    "        \n",
    "        return drift_summary\n",
    "    \n",
    "    def _calculate_psi(self, reference, current, n_bins=10):\n",
    "        \"\"\"PSI (Population Stability Index) 계산\"\"\"\n",
    "        # 빈 경계 생성\n",
    "        min_val = min(reference.min(), current.min())\n",
    "        max_val = max(reference.max(), current.max())\n",
    "        bins = np.linspace(min_val, max_val, n_bins + 1)\n",
    "        \n",
    "        # 히스토그램 계산\n",
    "        ref_counts, _ = np.histogram(reference, bins=bins)\n",
    "        cur_counts, _ = np.histogram(current, bins=bins)\n",
    "        \n",
    "        # 확률로 변환\n",
    "        ref_probs = (ref_counts + 1) / (len(reference) + n_bins)\n",
    "        cur_probs = (cur_counts + 1) / (len(current) + n_bins)\n",
    "        \n",
    "        # PSI 계산\n",
    "        psi = np.sum((cur_probs - ref_probs) * np.log(cur_probs / ref_probs))\n",
    "        \n",
    "        return psi\n",
    "    \n",
    "    def monitor_prediction_performance(self, \n",
    "                                      y_true, \n",
    "                                      y_pred, \n",
    "                                      timestamp=None):\n",
    "        \"\"\"예측 성능 모니터링\"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now()\n",
    "        \n",
    "        metrics = {\n",
    "            'timestamp': timestamp,\n",
    "            'accuracy': accuracy_score(y_true, y_pred),\n",
    "            'f1_score': f1_score(y_true, y_pred, average='weighted'),\n",
    "            'samples': len(y_true)\n",
    "        }\n",
    "        \n",
    "        self.monitoring_metrics.append(metrics)\n",
    "        \n",
    "        # 성능 저하 감지\n",
    "        if len(self.monitoring_metrics) > 10:\n",
    "            recent_accuracy = np.mean(\n",
    "                [m['accuracy'] for m in self.monitoring_metrics[-5:]]\n",
    "            )\n",
    "            baseline_accuracy = np.mean(\n",
    "                [m['accuracy'] for m in self.monitoring_metrics[:5]]\n",
    "            )\n",
    "            \n",
    "            if recent_accuracy < baseline_accuracy * 0.95:\n",
    "                print(f\"⚠️ Performance degradation detected: \"\n",
    "                      f\"{recent_accuracy:.3f} < {baseline_accuracy*0.95:.3f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def visualize_monitoring(self):\n",
    "        \"\"\"모니터링 결과 시각화\"\"\"\n",
    "        if not self.monitoring_metrics:\n",
    "            print(\"No monitoring data available\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # 성능 메트릭 추이\n",
    "        metrics_df = pd.DataFrame(self.monitoring_metrics)\n",
    "        axes[0, 0].plot(metrics_df['accuracy'], label='Accuracy', marker='o')\n",
    "        axes[0, 0].plot(metrics_df['f1_score'], label='F1 Score', marker='s')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('Score')\n",
    "        axes[0, 0].set_title('Model Performance Over Time')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 샘플 수 추이\n",
    "        axes[0, 1].bar(range(len(metrics_df)), metrics_df['samples'], \n",
    "                      color='steelblue', alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Time')\n",
    "        axes[0, 1].set_ylabel('Number of Samples')\n",
    "        axes[0, 1].set_title('Prediction Volume')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 드리프트 히스토리\n",
    "        if self.drift_history:\n",
    "            drift_ratios = [d['drift_ratio'] for d in self.drift_history]\n",
    "            axes[1, 0].plot(drift_ratios, color='red', marker='o')\n",
    "            axes[1, 0].axhline(y=0.3, color='red', linestyle='--', \n",
    "                             alpha=0.5, label='Drift Threshold')\n",
    "            axes[1, 0].set_xlabel('Check Number')\n",
    "            axes[1, 0].set_ylabel('Drift Ratio')\n",
    "            axes[1, 0].set_title('Feature Drift Detection')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 최근 드리프트 상태\n",
    "        if self.drift_history:\n",
    "            latest_drift = self.drift_history[-1]\n",
    "            feature_status = [\n",
    "                1 if v['drift_detected'] else 0 \n",
    "                for v in latest_drift['details'].values()\n",
    "            ]\n",
    "            \n",
    "            axes[1, 1].bar(range(len(feature_status)), feature_status, \n",
    "                          color=['red' if x else 'green' for x in feature_status])\n",
    "            axes[1, 1].set_xlabel('Feature Index')\n",
    "            axes[1, 1].set_ylabel('Drift Detected')\n",
    "            axes[1, 1].set_title('Latest Feature Drift Status')\n",
    "            axes[1, 1].set_ylim([0, 1.5])\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 모니터링 시뮬레이션\n",
    "print(\"Model Monitoring Simulation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "monitor = ModelMonitoring()\n",
    "\n",
    "# 참조 데이터 설정\n",
    "monitor.set_reference_data(X_train)\n",
    "\n",
    "# 시간에 따른 데이터 변화 시뮬레이션\n",
    "for i in range(5):\n",
    "    # 데이터 생성 (점진적 드리프트)\n",
    "    drift_factor = i * 0.5\n",
    "    X_current = X_test + np.random.normal(drift_factor, 0.5, X_test.shape)\n",
    "    \n",
    "    # 드리프트 감지\n",
    "    drift_result = monitor.detect_data_drift(X_current)\n",
    "    \n",
    "    print(f\"\\nTime step {i+1}:\")\n",
    "    print(f\"  Drift detected: {drift_result['overall_drift']}\")\n",
    "    print(f\"  Drifted features: {drift_result['drifted_features']}/{drift_result['total_features']}\")\n",
    "    \n",
    "    # 성능 모니터링 (시뮬레이션)\n",
    "    y_pred_sim = model.predict(X_test)\n",
    "    # 성능 저하 시뮬레이션\n",
    "    if i > 2:\n",
    "        # 일부 예측 오류 주입\n",
    "        error_idx = np.random.choice(len(y_pred_sim), size=int(len(y_pred_sim)*0.1))\n",
    "        y_pred_sim[error_idx] = (y_pred_sim[error_idx] + 1) % 3\n",
    "    \n",
    "    perf_metrics = monitor.monitor_prediction_performance(y_test, y_pred_sim)\n",
    "    print(f\"  Accuracy: {perf_metrics['accuracy']:.3f}\")\n",
    "\n",
    "# 모니터링 시각화\n",
    "monitor.visualize_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A/B 테스트 및 섀도우 배포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeploymentStrategy:\n",
    "    \"\"\"배포 전략 구현\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.traffic_split = {}\n",
    "        self.ab_test_results = []\n",
    "        \n",
    "    def canary_deployment(self, \n",
    "                         new_model, \n",
    "                         old_model,\n",
    "                         initial_traffic=0.1,\n",
    "                         increment=0.1,\n",
    "                         success_threshold=0.95):\n",
    "        \"\"\"카나리 배포 시뮬레이션\"\"\"\n",
    "        \n",
    "        current_traffic = initial_traffic\n",
    "        deployment_log = []\n",
    "        \n",
    "        print(\"Starting Canary Deployment\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        while current_traffic <= 1.0:\n",
    "            # 트래픽 분배\n",
    "            n_requests = 1000\n",
    "            n_new = int(n_requests * current_traffic)\n",
    "            n_old = n_requests - n_new\n",
    "            \n",
    "            # 성능 시뮬레이션\n",
    "            new_success_rate = np.random.uniform(0.93, 0.98)\n",
    "            old_success_rate = np.random.uniform(0.90, 0.95)\n",
    "            \n",
    "            # 메트릭 계산\n",
    "            overall_success = (\n",
    "                new_success_rate * n_new + \n",
    "                old_success_rate * n_old\n",
    "            ) / n_requests\n",
    "            \n",
    "            log_entry = {\n",
    "                'traffic_split': current_traffic,\n",
    "                'new_model_requests': n_new,\n",
    "                'old_model_requests': n_old,\n",
    "                'new_model_success': new_success_rate,\n",
    "                'old_model_success': old_success_rate,\n",
    "                'overall_success': overall_success\n",
    "            }\n",
    "            \n",
    "            deployment_log.append(log_entry)\n",
    "            \n",
    "            print(f\"Traffic: {current_traffic:.0%} to new model\")\n",
    "            print(f\"  New model success: {new_success_rate:.3f}\")\n",
    "            print(f\"  Old model success: {old_success_rate:.3f}\")\n",
    "            print(f\"  Overall success: {overall_success:.3f}\")\n",
    "            \n",
    "            # 성공 기준 확인\n",
    "            if new_success_rate < success_threshold:\n",
    "                print(f\"❌ Rollback! New model below threshold: \"\n",
    "                      f\"{new_success_rate:.3f} < {success_threshold}\")\n",
    "                return False, deployment_log\n",
    "            \n",
    "            print(f\"✓ Proceeding to next stage\\n\")\n",
    "            current_traffic = min(1.0, current_traffic + increment)\n",
    "        \n",
    "        print(\"✅ Canary deployment successful!\")\n",
    "        return True, deployment_log\n",
    "    \n",
    "    def ab_test(self, \n",
    "               model_a, \n",
    "               model_b,\n",
    "               n_samples=1000,\n",
    "               significance_level=0.05):\n",
    "        \"\"\"A/B 테스트 수행\"\"\"\n",
    "        \n",
    "        # 성능 시뮬레이션\n",
    "        success_a = np.random.binomial(n_samples, 0.92)\n",
    "        success_b = np.random.binomial(n_samples, 0.94)\n",
    "        \n",
    "        rate_a = success_a / n_samples\n",
    "        rate_b = success_b / n_samples\n",
    "        \n",
    "        # 비율 차이 검정 (z-test)\n",
    "        pooled_rate = (success_a + success_b) / (2 * n_samples)\n",
    "        se = np.sqrt(pooled_rate * (1 - pooled_rate) * (2 / n_samples))\n",
    "        z_score = (rate_b - rate_a) / se\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
    "        \n",
    "        # 신뢰구간 계산\n",
    "        ci_margin = 1.96 * se\n",
    "        ci_lower = (rate_b - rate_a) - ci_margin\n",
    "        ci_upper = (rate_b - rate_a) + ci_margin\n",
    "        \n",
    "        result = {\n",
    "            'model_a_success_rate': rate_a,\n",
    "            'model_b_success_rate': rate_b,\n",
    "            'difference': rate_b - rate_a,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < significance_level,\n",
    "            'confidence_interval': (ci_lower, ci_upper),\n",
    "            'winner': 'Model B' if rate_b > rate_a and p_value < significance_level else 'No winner'\n",
    "        }\n",
    "        \n",
    "        self.ab_test_results.append(result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def shadow_deployment(self, primary_model, shadow_model, n_requests=100):\n",
    "        \"\"\"섀도우 배포 (미러링)\"\"\"\n",
    "        \n",
    "        shadow_results = []\n",
    "        \n",
    "        for i in range(n_requests):\n",
    "            # 실제 요청 시뮬레이션\n",
    "            request_data = np.random.randn(10)  # 10 features\n",
    "            \n",
    "            # Primary 모델 예측\n",
    "            primary_start = time.perf_counter()\n",
    "            primary_pred = np.random.choice([0, 1, 2])  # 시뮬레이션\n",
    "            primary_latency = (time.perf_counter() - primary_start) * 1000\n",
    "            \n",
    "            # Shadow 모델 예측 (비동기적으로 실행)\n",
    "            shadow_start = time.perf_counter()\n",
    "            shadow_pred = np.random.choice([0, 1, 2])  # 시뮬레이션\n",
    "            shadow_latency = (time.perf_counter() - shadow_start) * 1000\n",
    "            \n",
    "            # 결과 비교\n",
    "            agreement = primary_pred == shadow_pred\n",
    "            \n",
    "            shadow_results.append({\n",
    "                'request_id': i,\n",
    "                'primary_prediction': primary_pred,\n",
    "                'shadow_prediction': shadow_pred,\n",
    "                'agreement': agreement,\n",
    "                'primary_latency_ms': primary_latency,\n",
    "                'shadow_latency_ms': shadow_latency\n",
    "            })\n",
    "        \n",
    "        # 통계 계산\n",
    "        df = pd.DataFrame(shadow_results)\n",
    "        \n",
    "        summary = {\n",
    "            'total_requests': n_requests,\n",
    "            'agreement_rate': df['agreement'].mean(),\n",
    "            'primary_avg_latency': df['primary_latency_ms'].mean(),\n",
    "            'shadow_avg_latency': df['shadow_latency_ms'].mean(),\n",
    "            'latency_difference': df['shadow_latency_ms'].mean() - df['primary_latency_ms'].mean()\n",
    "        }\n",
    "        \n",
    "        return summary, df\n",
    "\n",
    "# 배포 전략 시뮬레이션\n",
    "deployment = DeploymentStrategy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEPLOYMENT STRATEGY SIMULATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Canary Deployment\n",
    "print(\"\\n1. CANARY DEPLOYMENT\")\n",
    "print(\"-\"*40)\n",
    "success, canary_log = deployment.canary_deployment(\n",
    "    new_model=\"model_v2\",\n",
    "    old_model=\"model_v1\",\n",
    "    initial_traffic=0.1,\n",
    "    increment=0.2\n",
    ")\n",
    "\n",
    "# 2. A/B Testing\n",
    "print(\"\\n2. A/B TESTING\")\n",
    "print(\"-\"*40)\n",
    "ab_result = deployment.ab_test(\n",
    "    model_a=\"model_v1\",\n",
    "    model_b=\"model_v2\",\n",
    "    n_samples=2000\n",
    ")\n",
    "\n",
    "print(f\"Model A success rate: {ab_result['model_a_success_rate']:.3f}\")\n",
    "print(f\"Model B success rate: {ab_result['model_b_success_rate']:.3f}\")\n",
    "print(f\"Difference: {ab_result['difference']:.3f}\")\n",
    "print(f\"P-value: {ab_result['p_value']:.4f}\")\n",
    "print(f\"Statistically significant: {ab_result['significant']}\")\n",
    "print(f\"95% CI: [{ab_result['confidence_interval'][0]:.3f}, \"\n",
    "      f\"{ab_result['confidence_interval'][1]:.3f}]\")\n",
    "print(f\"Winner: {ab_result['winner']}\")\n",
    "\n",
    "# 3. Shadow Deployment\n",
    "print(\"\\n3. SHADOW DEPLOYMENT\")\n",
    "print(\"-\"*40)\n",
    "shadow_summary, shadow_df = deployment.shadow_deployment(\n",
    "    primary_model=\"model_v1\",\n",
    "    shadow_model=\"model_v2\",\n",
    "    n_requests=100\n",
    ")\n",
    "\n",
    "print(f\"Total requests: {shadow_summary['total_requests']}\")\n",
    "print(f\"Agreement rate: {shadow_summary['agreement_rate']:.1%}\")\n",
    "print(f\"Primary avg latency: {shadow_summary['primary_avg_latency']:.2f} ms\")\n",
    "print(f\"Shadow avg latency: {shadow_summary['shadow_avg_latency']:.2f} ms\")\n",
    "print(f\"Latency difference: {shadow_summary['latency_difference']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 과제\n",
    "\n",
    "1. **REST API 구현**: FastAPI를 사용해 실제 모델 서빙 API 구축\n",
    "2. **모델 최적화**: ONNX/TensorRT로 모델을 변환하고 성능 비교\n",
    "3. **MLflow 통합**: MLflow로 실험 추적 및 모델 레지스트리 구축\n",
    "4. **모니터링 대시보드**: Grafana/Prometheus로 실시간 모니터링 구현\n",
    "5. **자동 재학습 파이프라인**: 드리프트 감지시 자동 재학습 트리거"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}