{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSoq8CtFhBQq"
      },
      "source": [
        "# ì œì¡° AI ì‹¤ìŠµ ê³¼ì • - Session 3 (4-6ì‹œê°„)\n",
        "## ì˜ˆì§€ë³´ì „(ì‹œê³„ì—´) ì‹¬í™”\n",
        "\n",
        "### í•™ìŠµ ëª©í‘œ\n",
        "- ìœˆë„ìš°ë³„ FFT ì£¼ìš” í†µê³„í”¼ì²˜(RMS/Kurtosis) ì¶”ì¶œ ë° ë¶„ì„\n",
        "- Isolation Forest/OCSVM vs LSTM/AE ë¹„êµ ì‹¤ìŠµ\n",
        "- ê²½ë³´ ì •ì±… ì•ŒëŒ í”¼ë¡œë„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•œê¸€ ê¹¨ì§ ë°©ì§€\n",
        "\n",
        "Step 1: ì‹¤í–‰"
      ],
      "metadata": {
        "id": "kH4nwskyhFg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y fonts-nanum*\n",
        "!rm -rf /root/.cache/matplotlib/* # í°íŠ¸ ìºì‹œ ì¬ì„¤ì •\n",
        "print(\"=\" * 50)\n",
        "print(\"âœ… í°íŠ¸ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
        "print(\"âš ï¸ ì´ì œ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:\")\n",
        "print(\"1. ìƒë‹¨ ë©”ë‰´: ëŸ°íƒ€ì„ â†’ ëŸ°íƒ€ì„ ë‹¤ì‹œ ì‹œì‘\")\n",
        "print(\"2. ì¬ì‹œì‘ í›„ Step 2 ì½”ë“œ ì‹¤í–‰\")\n",
        "print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "AE41pyGQhGS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: ì‹¤í–‰"
      ],
      "metadata": {
        "id": "jZNpJ715hhMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "#\n",
        "path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
        "font_name = mpl.font_manager.FontProperties(fname=path).get_name()\n",
        "plt.rcParams['font.family'] = font_name\n",
        "\n",
        "# í™•ì¸\n",
        "plt.plot([1,2,3], [1,2,3])\n",
        "plt.title('í•œê¸€ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ğŸ‰')\n",
        "plt.xlabel('í•œê¸€ Xì¶•')\n",
        "plt.ylabel('í•œê¸€ Yì¶•')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DPTN1PBqhoh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59PCd1IlhBQr"
      },
      "source": [
        "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-5AjAAfhBQr"
      },
      "outputs": [],
      "source": [
        "# í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install scipy numpy pandas matplotlib seaborn -q\n",
        "!pip install scikit-learn -q\n",
        "!pip install tensorflow -q\n",
        "!pip install pywavelets -q  # Wavelet Transform\n",
        "!pip install statsmodels -q\n",
        "!pip install pyod -q  # Outlier Detection\n",
        "!pip install streamz -q  # Streaming data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edYoeg-3hBQr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Signal Processing\n",
        "from scipy import signal\n",
        "from scipy.fft import fft, fftfreq\n",
        "from scipy.stats import kurtosis, skew\n",
        "import pywt\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Visualization settings\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Check GPU\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
        "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10SGABgGhBQs"
      },
      "source": [
        "## 2. ì„¤ë¹„ ì‹œê³„ì—´ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜\n",
        "\n",
        "### 2.1 ë² ì–´ë§ ì§„ë™ ë°ì´í„° ìƒì„±"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDmx-1wzhBQs"
      },
      "outputs": [],
      "source": [
        "class BearingVibrationSimulator:\n",
        "    \"\"\"\n",
        "    ë² ì–´ë§ ì§„ë™ ë°ì´í„° ì‹œë®¬ë ˆì´í„°\n",
        "    ì •ìƒ â†’ ì´ˆê¸° ê²°í•¨ â†’ ì§„í–‰ëœ ê²°í•¨ â†’ ê³ ì¥ ë‹¨ê³„ë¥¼ ì‹œë®¬ë ˆì´ì…˜\n",
        "    \"\"\"\n",
        "    def __init__(self, sampling_rate=10000):  # 10 kHz\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.bearing_frequencies = {\n",
        "            'BPFO': 236.4,  # Ball Pass Frequency Outer race\n",
        "            'BPFI': 296.8,  # Ball Pass Frequency Inner race\n",
        "            'BSF': 139.9,   # Ball Spin Frequency\n",
        "            'FTF': 14.8     # Fundamental Train Frequency\n",
        "        }\n",
        "\n",
        "    def generate_normal_vibration(self, duration=10):\n",
        "        \"\"\"ì •ìƒ ì§„ë™ ì‹ í˜¸ ìƒì„±\"\"\"\n",
        "        t = np.linspace(0, duration, int(self.sampling_rate * duration))\n",
        "\n",
        "        # ê¸°ë³¸ íšŒì „ ì£¼íŒŒìˆ˜ (30 Hz = 1800 RPM)\n",
        "        rotation_freq = 30\n",
        "\n",
        "        # ì •ìƒ ì‹ í˜¸: íšŒì „ ì£¼íŒŒìˆ˜ + ëœë¤ ë…¸ì´ì¦ˆ\n",
        "        signal = (\n",
        "            0.5 * np.sin(2 * np.pi * rotation_freq * t) +  # 1X\n",
        "            0.2 * np.sin(2 * np.pi * 2 * rotation_freq * t) +  # 2X\n",
        "            0.1 * np.sin(2 * np.pi * 3 * rotation_freq * t) +  # 3X\n",
        "            0.3 * np.random.normal(0, 0.1, len(t))  # ë…¸ì´ì¦ˆ\n",
        "        )\n",
        "\n",
        "        return t, signal\n",
        "\n",
        "    def generate_faulty_vibration(self, duration=10, fault_type='outer_race', severity='early'):\n",
        "        \"\"\"ê²°í•¨ ì§„ë™ ì‹ í˜¸ ìƒì„±\"\"\"\n",
        "        t, normal_signal = self.generate_normal_vibration(duration)\n",
        "\n",
        "        # ê²°í•¨ ì£¼íŒŒìˆ˜ ì„ íƒ\n",
        "        if fault_type == 'outer_race':\n",
        "            fault_freq = self.bearing_frequencies['BPFO']\n",
        "        elif fault_type == 'inner_race':\n",
        "            fault_freq = self.bearing_frequencies['BPFI']\n",
        "        elif fault_type == 'ball':\n",
        "            fault_freq = self.bearing_frequencies['BSF']\n",
        "        else:\n",
        "            fault_freq = self.bearing_frequencies['FTF']\n",
        "\n",
        "        # ê²°í•¨ ì‹¬ê°ë„ì— ë”°ë¥¸ ì§„í­ ì„¤ì •\n",
        "        severity_amplitude = {\n",
        "            'early': 0.3,\n",
        "            'moderate': 0.8,\n",
        "            'severe': 2.0\n",
        "        }\n",
        "\n",
        "        amp = severity_amplitude.get(severity, 0.5)\n",
        "\n",
        "        # ì¶©ê²© ì‹ í˜¸ ìƒì„± (ê²°í•¨ íŠ¹ì„±)\n",
        "        impact_interval = 1 / fault_freq\n",
        "        impacts = np.zeros_like(t)\n",
        "\n",
        "        for i in range(int(duration / impact_interval)):\n",
        "            impact_time = i * impact_interval\n",
        "            impact_idx = int(impact_time * self.sampling_rate)\n",
        "            if impact_idx < len(impacts):\n",
        "                # ì§€ìˆ˜ ê°ì‡  ì¶©ê²© ì‹ í˜¸\n",
        "                decay_length = min(100, len(impacts) - impact_idx)\n",
        "                impacts[impact_idx:impact_idx+decay_length] += (\n",
        "                    amp * np.exp(-np.arange(decay_length) * 0.1) *\n",
        "                    np.sin(2 * np.pi * 2000 * np.arange(decay_length) / self.sampling_rate)\n",
        "                )\n",
        "\n",
        "        # ê²°í•¨ ì‹ í˜¸ = ì •ìƒ ì‹ í˜¸ + ì¶©ê²© ì‹ í˜¸\n",
        "        faulty_signal = normal_signal + impacts\n",
        "\n",
        "        # ê²°í•¨ì´ ì§„í–‰ë ìˆ˜ë¡ ë…¸ì´ì¦ˆ ì¦ê°€\n",
        "        if severity == 'severe':\n",
        "            faulty_signal += np.random.normal(0, 0.3, len(t))\n",
        "\n",
        "        return t, faulty_signal\n",
        "\n",
        "    def generate_degradation_sequence(self, total_days=30, samples_per_day=10):\n",
        "        \"\"\"ì„¤ë¹„ ì—´í™” ê³¼ì • ì‹œë®¬ë ˆì´ì…˜\"\"\"\n",
        "        degradation_data = []\n",
        "        labels = []\n",
        "        timestamps = []\n",
        "\n",
        "        for day in range(total_days):\n",
        "            # ì—´í™” ë‹¨ê³„ ê²°ì •\n",
        "            if day < total_days * 0.3:\n",
        "                stage = 'normal'\n",
        "                label = 0\n",
        "            elif day < total_days * 0.6:\n",
        "                stage = 'early'\n",
        "                label = 1\n",
        "            elif day < total_days * 0.85:\n",
        "                stage = 'moderate'\n",
        "                label = 2\n",
        "            else:\n",
        "                stage = 'severe'\n",
        "                label = 3\n",
        "\n",
        "            for sample in range(samples_per_day):\n",
        "                timestamp = datetime.now() + timedelta(days=day, hours=sample*2)\n",
        "\n",
        "                if stage == 'normal':\n",
        "                    _, signal = self.generate_normal_vibration(duration=1)\n",
        "                else:\n",
        "                    _, signal = self.generate_faulty_vibration(\n",
        "                        duration=1,\n",
        "                        fault_type='outer_race',\n",
        "                        severity=stage\n",
        "                    )\n",
        "\n",
        "                degradation_data.append(signal)\n",
        "                labels.append(label)\n",
        "                timestamps.append(timestamp)\n",
        "\n",
        "        return np.array(degradation_data), np.array(labels), timestamps\n",
        "\n",
        "# ë² ì–´ë§ ì§„ë™ ë°ì´í„° ìƒì„±\n",
        "simulator = BearingVibrationSimulator()\n",
        "\n",
        "# ì •ìƒ vs ê²°í•¨ ì‹ í˜¸ ë¹„êµ\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
        "fig.suptitle('ë² ì–´ë§ ì§„ë™ ì‹ í˜¸: ì •ìƒ vs ê²°í•¨ ë‹¨ê³„', fontsize=16, fontweight='bold')\n",
        "\n",
        "# ì‹œê°„ ë„ë©”ì¸ ì‹ í˜¸\n",
        "stages = [('normal', None), ('early', 'early'), ('severe', 'severe')]\n",
        "\n",
        "for idx, (ax_time, ax_freq) in enumerate(axes):\n",
        "    stage_name, severity = stages[idx]\n",
        "\n",
        "    if stage_name == 'normal':\n",
        "        t, signal = simulator.generate_normal_vibration(duration=0.5)\n",
        "        title = 'Normal Operation'\n",
        "    else:\n",
        "        t, signal = simulator.generate_faulty_vibration(duration=0.5, severity=severity)\n",
        "        title = f'Fault: {severity.capitalize()} Stage'\n",
        "\n",
        "    # Time domain\n",
        "    ax_time.plot(t[:5000], signal[:5000], linewidth=0.5)\n",
        "    ax_time.set_title(f'{title} - Time Domain')\n",
        "    ax_time.set_xlabel('Time (s)')\n",
        "    ax_time.set_ylabel('Amplitude (g)')\n",
        "    ax_time.grid(True, alpha=0.3)\n",
        "\n",
        "    # Frequency domain (FFT)\n",
        "    N = len(signal)\n",
        "    yf = fft(signal)\n",
        "    xf = fftfreq(N, 1/simulator.sampling_rate)[:N//2]\n",
        "\n",
        "    ax_freq.semilogy(xf[:2000], 2.0/N * np.abs(yf[:2000]))\n",
        "    ax_freq.set_title(f'{title} - Frequency Domain')\n",
        "    ax_freq.set_xlabel('Frequency (Hz)')\n",
        "    ax_freq.set_ylabel('Magnitude')\n",
        "    ax_freq.grid(True, alpha=0.3)\n",
        "\n",
        "    # ê²°í•¨ ì£¼íŒŒìˆ˜ í‘œì‹œ\n",
        "    if stage_name != 'normal':\n",
        "        ax_freq.axvline(x=simulator.bearing_frequencies['BPFO'],\n",
        "                       color='r', linestyle='--', alpha=0.5, label='BPFO')\n",
        "        ax_freq.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… ë² ì–´ë§ ì§„ë™ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ\")\n",
        "print(f\"   - Sampling Rate: {simulator.sampling_rate} Hz\")\n",
        "print(f\"   - Bearing Fault Frequencies:\")\n",
        "for fault, freq in simulator.bearing_frequencies.items():\n",
        "    print(f\"     {fault}: {freq:.1f} Hz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npo88Az_hBQs"
      },
      "source": [
        "## 3. íŠ¹ì§• ì¶”ì¶œ (Feature Engineering)\n",
        "\n",
        "### 3.1 ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§• ì¶”ì¶œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q9A6OXRhBQs"
      },
      "outputs": [],
      "source": [
        "class TimeFeatureExtractor:\n",
        "    \"\"\"ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§• ì¶”ì¶œê¸°\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_statistical_features(signal):\n",
        "        \"\"\"í†µê³„ì  íŠ¹ì§• ì¶”ì¶œ\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # ê¸°ë³¸ í†µê³„ëŸ‰\n",
        "        features['mean'] = np.mean(signal)\n",
        "        features['std'] = np.std(signal)\n",
        "        features['max'] = np.max(signal)\n",
        "        features['min'] = np.min(signal)\n",
        "        features['rms'] = np.sqrt(np.mean(signal**2))  # Root Mean Square\n",
        "\n",
        "        # ê³ ì°¨ í†µê³„ëŸ‰\n",
        "        features['skewness'] = skew(signal)\n",
        "        features['kurtosis'] = kurtosis(signal)  # ì¶©ê²© ì‹ í˜¸ ê°ì§€ì— ìœ ìš©\n",
        "\n",
        "        # Peak ê´€ë ¨ íŠ¹ì§•\n",
        "        features['peak'] = np.max(np.abs(signal))\n",
        "        features['peak_to_peak'] = features['max'] - features['min']\n",
        "        features['crest_factor'] = features['peak'] / features['rms'] if features['rms'] != 0 else 0\n",
        "\n",
        "        # Shape Factor\n",
        "        features['shape_factor'] = features['rms'] / np.mean(np.abs(signal)) if np.mean(np.abs(signal)) != 0 else 0\n",
        "\n",
        "        # Impulse Factor\n",
        "        features['impulse_factor'] = features['peak'] / np.mean(np.abs(signal)) if np.mean(np.abs(signal)) != 0 else 0\n",
        "\n",
        "        # Clearance Factor\n",
        "        features['clearance_factor'] = features['peak'] / (np.mean(np.sqrt(np.abs(signal)))**2) if np.mean(np.sqrt(np.abs(signal))) != 0 else 0\n",
        "\n",
        "        return features\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_waveform_features(signal):\n",
        "        \"\"\"íŒŒí˜• íŠ¹ì§• ì¶”ì¶œ\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Zero Crossing Rate\n",
        "        zero_crossings = np.where(np.diff(np.signbit(signal)))[0]\n",
        "        features['zero_crossing_rate'] = len(zero_crossings) / len(signal)\n",
        "\n",
        "        # Mean Absolute Deviation\n",
        "        features['mad'] = np.mean(np.abs(signal - np.mean(signal)))\n",
        "\n",
        "        # Energy\n",
        "        features['energy'] = np.sum(signal**2)\n",
        "\n",
        "        # Shannon Entropy\n",
        "        hist, _ = np.histogram(signal, bins=50)\n",
        "        hist = hist / hist.sum()\n",
        "        hist = hist[hist > 0]  # Remove zeros\n",
        "        features['entropy'] = -np.sum(hist * np.log2(hist))\n",
        "\n",
        "        return features\n",
        "\n",
        "# ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§• ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
        "time_extractor = TimeFeatureExtractor()\n",
        "\n",
        "# ì •ìƒ ì‹ í˜¸ì™€ ê²°í•¨ ì‹ í˜¸ ë¹„êµ\n",
        "_, normal_signal = simulator.generate_normal_vibration(duration=1)\n",
        "_, faulty_signal = simulator.generate_faulty_vibration(duration=1, severity='severe')\n",
        "\n",
        "normal_features = time_extractor.extract_statistical_features(normal_signal)\n",
        "faulty_features = time_extractor.extract_statistical_features(faulty_signal)\n",
        "\n",
        "# íŠ¹ì§• ë¹„êµ ì‹œê°í™”\n",
        "feature_comparison = pd.DataFrame({\n",
        "    'Normal': normal_features,\n",
        "    'Faulty': faulty_features\n",
        "})\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Feature comparison bar plot\n",
        "feature_comparison.T.plot(kind='bar', ax=axes[0])\n",
        "axes[0].set_title('Time Domain Features: Normal vs Faulty')\n",
        "axes[0].set_xlabel('Condition')\n",
        "axes[0].set_ylabel('Feature Value')\n",
        "axes[0].legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Feature importance (relative change)\n",
        "relative_change = ((feature_comparison['Faulty'] - feature_comparison['Normal']) /\n",
        "                  feature_comparison['Normal'].abs() * 100)\n",
        "relative_change = relative_change.sort_values(ascending=True)\n",
        "\n",
        "axes[1].barh(range(len(relative_change)), relative_change.values)\n",
        "axes[1].set_yticks(range(len(relative_change)))\n",
        "axes[1].set_yticklabels(relative_change.index)\n",
        "axes[1].set_xlabel('Relative Change (%)')\n",
        "axes[1].set_title('Feature Sensitivity to Fault')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ“Š ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§• ë¹„êµ:\")\n",
        "print(feature_comparison.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qHh2tTshBQs"
      },
      "source": [
        "### 3.2 ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì§• ì¶”ì¶œ (FFT ê¸°ë°˜)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eoZ48wfhBQt"
      },
      "outputs": [],
      "source": [
        "class FrequencyFeatureExtractor:\n",
        "    \"\"\"ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì§• ì¶”ì¶œê¸°\"\"\"\n",
        "\n",
        "    def __init__(self, sampling_rate):\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "    def extract_fft_features(self, signal, n_bands=10):\n",
        "        \"\"\"FFT ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # FFT ê³„ì‚°\n",
        "        N = len(signal)\n",
        "        yf = fft(signal)\n",
        "        xf = fftfreq(N, 1/self.sampling_rate)[:N//2]\n",
        "        magnitude = 2.0/N * np.abs(yf[:N//2])\n",
        "\n",
        "        # ì£¼íŒŒìˆ˜ ëŒ€ì—­ ë‚˜ëˆ„ê¸°\n",
        "        freq_bands = np.linspace(0, self.sampling_rate//2, n_bands+1)\n",
        "\n",
        "        # ê° ëŒ€ì—­ì˜ ì—ë„ˆì§€ ê³„ì‚°\n",
        "        for i in range(n_bands):\n",
        "            band_mask = (xf >= freq_bands[i]) & (xf < freq_bands[i+1])\n",
        "            features[f'band_{i}_energy'] = np.sum(magnitude[band_mask]**2)\n",
        "\n",
        "        # ì „ì²´ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§•\n",
        "        features['total_power'] = np.sum(magnitude**2)\n",
        "        features['mean_frequency'] = np.sum(xf * magnitude) / np.sum(magnitude) if np.sum(magnitude) > 0 else 0\n",
        "\n",
        "        # Peak frequency\n",
        "        peak_idx = np.argmax(magnitude)\n",
        "        features['peak_frequency'] = xf[peak_idx]\n",
        "        features['peak_magnitude'] = magnitude[peak_idx]\n",
        "\n",
        "        # Spectral entropy\n",
        "        psd = magnitude**2 / np.sum(magnitude**2)\n",
        "        psd = psd[psd > 0]\n",
        "        features['spectral_entropy'] = -np.sum(psd * np.log2(psd))\n",
        "\n",
        "        # Spectral centroid (ë¬´ê²Œì¤‘ì‹¬)\n",
        "        features['spectral_centroid'] = np.sum(xf * magnitude) / np.sum(magnitude) if np.sum(magnitude) > 0 else 0\n",
        "\n",
        "        # Spectral spread\n",
        "        if features['spectral_centroid'] > 0:\n",
        "            features['spectral_spread'] = np.sqrt(\n",
        "                np.sum(((xf - features['spectral_centroid'])**2) * magnitude) / np.sum(magnitude)\n",
        "            )\n",
        "        else:\n",
        "            features['spectral_spread'] = 0\n",
        "\n",
        "        return features, xf, magnitude\n",
        "\n",
        "    def extract_envelope_spectrum(self, signal):\n",
        "        \"\"\"Envelope spectrum ë¶„ì„ (ë² ì–´ë§ ê²°í•¨ ê°ì§€ì— íš¨ê³¼ì )\"\"\"\n",
        "        # Hilbert transformìœ¼ë¡œ envelope ì¶”ì¶œ\n",
        "        analytic_signal = signal.hilbert(signal)\n",
        "        envelope = np.abs(analytic_signal)\n",
        "\n",
        "        # Envelopeì˜ FFT\n",
        "        N = len(envelope)\n",
        "        yf = fft(envelope)\n",
        "        xf = fftfreq(N, 1/self.sampling_rate)[:N//2]\n",
        "        magnitude = 2.0/N * np.abs(yf[:N//2])\n",
        "\n",
        "        return xf, magnitude\n",
        "\n",
        "# ì£¼íŒŒìˆ˜ íŠ¹ì§• ì¶”ì¶œ\n",
        "freq_extractor = FrequencyFeatureExtractor(simulator.sampling_rate)\n",
        "\n",
        "# Windowë³„ FFT íŠ¹ì§• ì¶”ì¶œ\n",
        "def extract_windowed_features(signal, window_size=1024, overlap=0.5):\n",
        "    \"\"\"ìœˆë„ìš° ë‹¨ìœ„ë¡œ íŠ¹ì§• ì¶”ì¶œ\"\"\"\n",
        "    step_size = int(window_size * (1 - overlap))\n",
        "    n_windows = (len(signal) - window_size) // step_size + 1\n",
        "\n",
        "    all_features = []\n",
        "\n",
        "    for i in range(n_windows):\n",
        "        start = i * step_size\n",
        "        end = start + window_size\n",
        "        window = signal[start:end]\n",
        "\n",
        "        # ì‹œê°„ ë„ë©”ì¸ íŠ¹ì§•\n",
        "        time_features = time_extractor.extract_statistical_features(window)\n",
        "\n",
        "        # ì£¼íŒŒìˆ˜ ë„ë©”ì¸ íŠ¹ì§•\n",
        "        freq_features, _, _ = freq_extractor.extract_fft_features(window)\n",
        "\n",
        "        # íŠ¹ì§• ê²°í•©\n",
        "        combined_features = {**time_features, **freq_features}\n",
        "        all_features.append(combined_features)\n",
        "\n",
        "    return pd.DataFrame(all_features)\n",
        "\n",
        "# ì—´í™” ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ\n",
        "degradation_signals, degradation_labels, timestamps = simulator.generate_degradation_sequence(\n",
        "    total_days=10, samples_per_day=5\n",
        ")\n",
        "\n",
        "# ê° ì‹ í˜¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ\n",
        "all_extracted_features = []\n",
        "for signal in degradation_signals:\n",
        "    features_df = extract_windowed_features(signal)\n",
        "    all_extracted_features.append(features_df.mean())  # í‰ê·  íŠ¹ì§• ì‚¬ìš©\n",
        "\n",
        "features_matrix = pd.DataFrame(all_extracted_features)\n",
        "features_matrix['label'] = degradation_labels\n",
        "features_matrix['timestamp'] = timestamps\n",
        "\n",
        "# ì£¼ìš” íŠ¹ì§• ë³€í™” ì‹œê°í™”\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "fig.suptitle('Feature Evolution During Equipment Degradation', fontsize=16, fontweight='bold')\n",
        "\n",
        "key_features = ['rms', 'kurtosis', 'crest_factor', 'peak_frequency', 'spectral_entropy', 'total_power']\n",
        "\n",
        "for ax, feature in zip(axes.flat, key_features):\n",
        "    if feature in features_matrix.columns:\n",
        "        ax.plot(range(len(features_matrix)), features_matrix[feature].values)\n",
        "\n",
        "        # ìƒíƒœë³„ ìƒ‰ìƒ êµ¬ë¶„\n",
        "        colors = ['green', 'yellow', 'orange', 'red']\n",
        "        for label in range(4):\n",
        "            mask = features_matrix['label'] == label\n",
        "            indices = np.where(mask)[0]\n",
        "            if len(indices) > 0:\n",
        "                ax.scatter(indices, features_matrix[feature].values[mask],\n",
        "                          c=colors[label], alpha=0.6, s=20)\n",
        "\n",
        "        ax.set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
        "        ax.set_xlabel('Sample Index')\n",
        "        ax.set_ylabel('Feature Value')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ… íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ\")\n",
        "print(f\"   - ì´ ìƒ˜í”Œ ìˆ˜: {len(features_matrix)}\")\n",
        "print(f\"   - ì¶”ì¶œëœ íŠ¹ì§• ìˆ˜: {len(features_matrix.columns) - 2}\")  # label, timestamp ì œì™¸\n",
        "print(f\"\\nìƒíƒœë³„ ìƒ˜í”Œ ë¶„í¬:\")\n",
        "print(features_matrix['label'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suH-wl_-hBQt"
      },
      "source": [
        "## 4. ì´ìƒ íƒì§€ ëª¨ë¸ ë¹„êµ\n",
        "\n",
        "### 4.1 Isolation Forest vs One-Class SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2jm5nqRhBQt"
      },
      "outputs": [],
      "source": [
        "class AnomalyDetectionComparison:\n",
        "    \"\"\"ì´ìƒ íƒì§€ ëª¨ë¸ ë¹„êµ í´ë˜ìŠ¤\"\"\"\n",
        "\n",
        "    def __init__(self, features_df, labels):\n",
        "        self.features = features_df.drop(['label', 'timestamp'], axis=1, errors='ignore')\n",
        "        self.labels = labels\n",
        "        self.scaler = StandardScaler()\n",
        "        self.features_scaled = self.scaler.fit_transform(self.features)\n",
        "\n",
        "    def train_isolation_forest(self, contamination=0.1):\n",
        "        \"\"\"Isolation Forest í•™ìŠµ\"\"\"\n",
        "        model = IsolationForest(\n",
        "            contamination=contamination,\n",
        "            random_state=42,\n",
        "            n_estimators=100\n",
        "        )\n",
        "\n",
        "        # ì •ìƒ ë°ì´í„°ë¡œë§Œ í•™ìŠµ (label=0)\n",
        "        normal_data = self.features_scaled[self.labels == 0]\n",
        "        model.fit(normal_data)\n",
        "\n",
        "        # ì „ì²´ ë°ì´í„° ì˜ˆì¸¡\n",
        "        predictions = model.predict(self.features_scaled)\n",
        "        scores = model.score_samples(self.features_scaled)\n",
        "\n",
        "        return model, predictions, scores\n",
        "\n",
        "    def train_ocsvm(self, nu=0.1, kernel='rbf'):\n",
        "        \"\"\"One-Class SVM í•™ìŠµ\"\"\"\n",
        "        model = OneClassSVM(\n",
        "            nu=nu,\n",
        "            kernel=kernel,\n",
        "            gamma='auto'\n",
        "        )\n",
        "\n",
        "        # ì •ìƒ ë°ì´í„°ë¡œë§Œ í•™ìŠµ\n",
        "        normal_data = self.features_scaled[self.labels == 0]\n",
        "        model.fit(normal_data)\n",
        "\n",
        "        # ì „ì²´ ë°ì´í„° ì˜ˆì¸¡\n",
        "        predictions = model.predict(self.features_scaled)\n",
        "        scores = model.decision_function(self.features_scaled)\n",
        "\n",
        "        return model, predictions, scores\n",
        "\n",
        "    def evaluate_model(self, predictions, model_name):\n",
        "        \"\"\"ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\"\"\"\n",
        "        # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜ (0: ì •ìƒ, 1-3: ì´ìƒ)\n",
        "        binary_labels = (self.labels > 0).astype(int)\n",
        "        binary_predictions = (predictions == -1).astype(int)  # -1: ì´ìƒ, 1: ì •ìƒ\n",
        "\n",
        "        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\n",
        "        cm = confusion_matrix(binary_labels, binary_predictions)\n",
        "        report = classification_report(binary_labels, binary_predictions,\n",
        "                                      target_names=['Normal', 'Anomaly'])\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"{model_name} Performance\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(report)\n",
        "\n",
        "        return cm\n",
        "\n",
        "    def compare_models(self):\n",
        "        \"\"\"ëª¨ë¸ ë¹„êµ ì‹œê°í™”\"\"\"\n",
        "        # ëª¨ë¸ í•™ìŠµ\n",
        "        if_model, if_pred, if_scores = self.train_isolation_forest()\n",
        "        ocsvm_model, ocsvm_pred, ocsvm_scores = self.train_ocsvm()\n",
        "\n",
        "        # í‰ê°€\n",
        "        if_cm = self.evaluate_model(if_pred, \"Isolation Forest\")\n",
        "        ocsvm_cm = self.evaluate_model(ocsvm_pred, \"One-Class SVM\")\n",
        "\n",
        "        # ì‹œê°í™”\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        fig.suptitle('Anomaly Detection Model Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Isolation Forest Results\n",
        "        axes[0, 0].plot(if_scores)\n",
        "        axes[0, 0].scatter(range(len(if_scores)), if_scores,\n",
        "                          c=['red' if x == -1 else 'blue' for x in if_pred],\n",
        "                          alpha=0.5, s=10)\n",
        "        axes[0, 0].set_title('Isolation Forest - Anomaly Scores')\n",
        "        axes[0, 0].set_xlabel('Sample Index')\n",
        "        axes[0, 0].set_ylabel('Anomaly Score')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # OCSVM Results\n",
        "        axes[1, 0].plot(ocsvm_scores)\n",
        "        axes[1, 0].scatter(range(len(ocsvm_scores)), ocsvm_scores,\n",
        "                          c=['red' if x == -1 else 'blue' for x in ocsvm_pred],\n",
        "                          alpha=0.5, s=10)\n",
        "        axes[1, 0].set_title('One-Class SVM - Decision Scores')\n",
        "        axes[1, 0].set_xlabel('Sample Index')\n",
        "        axes[1, 0].set_ylabel('Decision Score')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Confusion Matrices\n",
        "        sns.heatmap(if_cm, annot=True, fmt='d', ax=axes[0, 1], cmap='Blues')\n",
        "        axes[0, 1].set_title('Isolation Forest - Confusion Matrix')\n",
        "        axes[0, 1].set_xlabel('Predicted')\n",
        "        axes[0, 1].set_ylabel('Actual')\n",
        "\n",
        "        sns.heatmap(ocsvm_cm, annot=True, fmt='d', ax=axes[1, 1], cmap='Oranges')\n",
        "        axes[1, 1].set_title('One-Class SVM - Confusion Matrix')\n",
        "        axes[1, 1].set_xlabel('Predicted')\n",
        "        axes[1, 1].set_ylabel('Actual')\n",
        "\n",
        "        # Detection over time\n",
        "        axes[0, 2].plot(self.labels, 'g-', label='True Labels', alpha=0.7)\n",
        "        axes[0, 2].scatter(range(len(if_pred)),\n",
        "                          [3 if x == -1 else 0 for x in if_pred],\n",
        "                          c='red', alpha=0.5, s=10, label='IF Detections')\n",
        "        axes[0, 2].set_title('Isolation Forest - Detection Timeline')\n",
        "        axes[0, 2].set_xlabel('Time')\n",
        "        axes[0, 2].set_ylabel('State')\n",
        "        axes[0, 2].legend()\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[1, 2].plot(self.labels, 'g-', label='True Labels', alpha=0.7)\n",
        "        axes[1, 2].scatter(range(len(ocsvm_pred)),\n",
        "                          [3 if x == -1 else 0 for x in ocsvm_pred],\n",
        "                          c='orange', alpha=0.5, s=10, label='OCSVM Detections')\n",
        "        axes[1, 2].set_title('One-Class SVM - Detection Timeline')\n",
        "        axes[1, 2].set_xlabel('Time')\n",
        "        axes[1, 2].set_ylabel('State')\n",
        "        axes[1, 2].legend()\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return if_model, ocsvm_model\n",
        "\n",
        "# ì´ìƒ íƒì§€ ëª¨ë¸ ë¹„êµ ì‹¤í–‰\n",
        "anomaly_detector = AnomalyDetectionComparison(features_matrix, degradation_labels)\n",
        "if_model, ocsvm_model = anomaly_detector.compare_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEqHtfznhBQt"
      },
      "source": [
        "### 4.2 LSTM Autoencoder for Anomaly Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbmPL2CshBQt"
      },
      "outputs": [],
      "source": [
        "class LSTMAutoencoder:\n",
        "    \"\"\"LSTM ê¸°ë°˜ Autoencoder for ì‹œê³„ì—´ ì´ìƒ íƒì§€\"\"\"\n",
        "\n",
        "    def __init__(self, sequence_length=50, n_features=10):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.n_features = n_features\n",
        "        self.model = None\n",
        "        self.threshold = None\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"LSTM Autoencoder ëª¨ë¸ êµ¬ì„±\"\"\"\n",
        "        # Encoder\n",
        "        inputs = keras.Input(shape=(self.sequence_length, self.n_features))\n",
        "\n",
        "        # Encoder layers\n",
        "        encoded = layers.LSTM(128, activation='relu', return_sequences=True)(inputs)\n",
        "        encoded = layers.Dropout(0.2)(encoded)\n",
        "        encoded = layers.LSTM(64, activation='relu', return_sequences=True)(encoded)\n",
        "        encoded = layers.Dropout(0.2)(encoded)\n",
        "        encoded = layers.LSTM(32, activation='relu', return_sequences=False)(encoded)\n",
        "\n",
        "        # Bottleneck\n",
        "        encoded = layers.RepeatVector(self.sequence_length)(encoded)\n",
        "\n",
        "        # Decoder layers\n",
        "        decoded = layers.LSTM(32, activation='relu', return_sequences=True)(encoded)\n",
        "        decoded = layers.Dropout(0.2)(decoded)\n",
        "        decoded = layers.LSTM(64, activation='relu', return_sequences=True)(decoded)\n",
        "        decoded = layers.Dropout(0.2)(decoded)\n",
        "        decoded = layers.LSTM(128, activation='relu', return_sequences=True)(decoded)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = layers.TimeDistributed(layers.Dense(self.n_features))(decoded)\n",
        "\n",
        "        # Model\n",
        "        self.model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        self.model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def prepare_sequences(self, data, labels=None):\n",
        "        \"\"\"ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„\"\"\"\n",
        "        sequences = []\n",
        "        sequence_labels = [] if labels is not None else None\n",
        "\n",
        "        for i in range(len(data) - self.sequence_length + 1):\n",
        "            seq = data[i:i+self.sequence_length]\n",
        "            sequences.append(seq)\n",
        "\n",
        "            if labels is not None:\n",
        "                # ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ë¼ë²¨ ì‚¬ìš©\n",
        "                sequence_labels.append(labels[i+self.sequence_length-1])\n",
        "\n",
        "        return np.array(sequences), np.array(sequence_labels) if labels is not None else None\n",
        "\n",
        "    def train(self, normal_data, epochs=50, batch_size=32):\n",
        "        \"\"\"ì •ìƒ ë°ì´í„°ë¡œ í•™ìŠµ\"\"\"\n",
        "        # ëª¨ë¸ ë¹Œë“œ\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        # í•™ìŠµ\n",
        "        history = self.model.fit(\n",
        "            normal_data, normal_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=0.1,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # ì •ìƒ ë°ì´í„°ì˜ ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°\n",
        "        predictions = self.model.predict(normal_data, verbose=0)\n",
        "        mse = np.mean(np.square(normal_data - predictions), axis=(1, 2))\n",
        "\n",
        "        # ì„ê³„ê°’ ì„¤ì • (í‰ê·  + 3*í‘œì¤€í¸ì°¨)\n",
        "        self.threshold = np.mean(mse) + 3 * np.std(mse)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def detect_anomalies(self, data):\n",
        "        \"\"\"ì´ìƒ íƒì§€\"\"\"\n",
        "        predictions = self.model.predict(data, verbose=0)\n",
        "        mse = np.mean(np.square(data - predictions), axis=(1, 2))\n",
        "\n",
        "        # ì´ìƒ íŒì •\n",
        "        anomalies = mse > self.threshold\n",
        "\n",
        "        return anomalies, mse\n",
        "\n",
        "# LSTM Autoencoder í•™ìŠµ ë° í…ŒìŠ¤íŠ¸\n",
        "# íŠ¹ì§• ë°ì´í„° ì¤€ë¹„\n",
        "features_for_lstm = features_matrix.drop(['label', 'timestamp'], axis=1, errors='ignore').values\n",
        "scaler = MinMaxScaler()\n",
        "features_scaled = scaler.fit_transform(features_for_lstm)\n",
        "\n",
        "# ì‹œí€€ìŠ¤ ìƒì„±\n",
        "sequence_length = 10\n",
        "lstm_ae = LSTMAutoencoder(sequence_length=sequence_length, n_features=features_scaled.shape[1])\n",
        "sequences, sequence_labels = lstm_ae.prepare_sequences(features_scaled, degradation_labels)\n",
        "\n",
        "# ì •ìƒ ë°ì´í„°ë§Œ ì„ íƒ (label=0)\n",
        "normal_sequences = sequences[sequence_labels == 0]\n",
        "\n",
        "# ëª¨ë¸ í•™ìŠµ\n",
        "print(\"ğŸš€ LSTM Autoencoder í•™ìŠµ ì¤‘...\")\n",
        "history = lstm_ae.train(normal_sequences, epochs=30, batch_size=16)\n",
        "\n",
        "# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì´ìƒ íƒì§€\n",
        "anomalies, reconstruction_errors = lstm_ae.detect_anomalies(sequences)\n",
        "\n",
        "# ê²°ê³¼ ì‹œê°í™”\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('LSTM Autoencoder Anomaly Detection', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Training history\n",
        "axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[0, 0].set_title('Model Training History')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss (MSE)')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Reconstruction errors\n",
        "axes[0, 1].plot(reconstruction_errors)\n",
        "axes[0, 1].axhline(y=lstm_ae.threshold, color='r', linestyle='--', label='Threshold')\n",
        "axes[0, 1].scatter(np.where(anomalies)[0], reconstruction_errors[anomalies],\n",
        "                  color='red', alpha=0.6, s=20, label='Anomalies')\n",
        "axes[0, 1].set_title('Reconstruction Errors')\n",
        "axes[0, 1].set_xlabel('Sequence Index')\n",
        "axes[0, 1].set_ylabel('MSE')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Detection vs True labels\n",
        "axes[1, 0].plot(sequence_labels, 'g-', label='True Labels', alpha=0.7)\n",
        "axes[1, 0].scatter(np.where(anomalies)[0],\n",
        "                  [3] * sum(anomalies),\n",
        "                  color='red', alpha=0.5, s=10, label='LSTM-AE Detections')\n",
        "axes[1, 0].set_title('Anomaly Detection Timeline')\n",
        "axes[1, 0].set_xlabel('Time')\n",
        "axes[1, 0].set_ylabel('State')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion Matrix\n",
        "binary_labels = (sequence_labels > 0).astype(int)\n",
        "cm = confusion_matrix(binary_labels, anomalies)\n",
        "sns.heatmap(cm, annot=True, fmt='d', ax=axes[1, 1], cmap='Purples')\n",
        "axes[1, 1].set_title('LSTM-AE Confusion Matrix')\n",
        "axes[1, 1].set_xlabel('Predicted')\n",
        "axes[1, 1].set_ylabel('Actual')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ì„±ëŠ¥ ë¦¬í¬íŠ¸\n",
        "print(\"\\nğŸ“Š LSTM Autoencoder Performance:\")\n",
        "print(classification_report(binary_labels, anomalies,\n",
        "                           target_names=['Normal', 'Anomaly']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBRrFwHZhBQu"
      },
      "source": [
        "## 5. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "### 5.1 ì•ŒëŒ ì •ì±… ë° í”¼ë¡œë„ ê´€ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izlq_qc-hBQu"
      },
      "outputs": [],
      "source": [
        "class AlarmManagementSystem:\n",
        "    \"\"\"ì•ŒëŒ ê´€ë¦¬ ì‹œìŠ¤í…œ - ì•ŒëŒ í”¼ë¡œë„ ë°©ì§€\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.alarm_history = []\n",
        "        self.alarm_policies = {\n",
        "            'critical': {\n",
        "                'threshold': 0.9,\n",
        "                'min_interval': 60,  # seconds\n",
        "                'max_frequency': 5,  # per hour\n",
        "                'actions': ['email', 'sms', 'stop_line']\n",
        "            },\n",
        "            'high': {\n",
        "                'threshold': 0.7,\n",
        "                'min_interval': 300,\n",
        "                'max_frequency': 10,\n",
        "                'actions': ['email', 'dashboard']\n",
        "            },\n",
        "            'medium': {\n",
        "                'threshold': 0.5,\n",
        "                'min_interval': 600,\n",
        "                'max_frequency': 20,\n",
        "                'actions': ['dashboard']\n",
        "            },\n",
        "            'low': {\n",
        "                'threshold': 0.3,\n",
        "                'min_interval': 3600,\n",
        "                'max_frequency': 50,\n",
        "                'actions': ['log']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def evaluate_alarm(self, anomaly_score, timestamp, equipment_id):\n",
        "        \"\"\"ì•ŒëŒ í‰ê°€ ë° í•„í„°ë§\"\"\"\n",
        "        # ì•ŒëŒ ë ˆë²¨ ê²°ì •\n",
        "        alarm_level = self._determine_alarm_level(anomaly_score)\n",
        "\n",
        "        if alarm_level is None:\n",
        "            return None\n",
        "\n",
        "        # ì•ŒëŒ í”¼ë¡œë„ ì²´í¬\n",
        "        if self._check_alarm_fatigue(equipment_id, alarm_level, timestamp):\n",
        "            return None\n",
        "\n",
        "        # ì•ŒëŒ ìƒì„±\n",
        "        alarm = {\n",
        "            'timestamp': timestamp,\n",
        "            'equipment_id': equipment_id,\n",
        "            'level': alarm_level,\n",
        "            'score': anomaly_score,\n",
        "            'actions': self.alarm_policies[alarm_level]['actions']\n",
        "        }\n",
        "\n",
        "        self.alarm_history.append(alarm)\n",
        "\n",
        "        return alarm\n",
        "\n",
        "    def _determine_alarm_level(self, score):\n",
        "        \"\"\"ì•ŒëŒ ë ˆë²¨ ê²°ì •\"\"\"\n",
        "        for level in ['critical', 'high', 'medium', 'low']:\n",
        "            if score >= self.alarm_policies[level]['threshold']:\n",
        "                return level\n",
        "        return None\n",
        "\n",
        "    def _check_alarm_fatigue(self, equipment_id, level, timestamp):\n",
        "        \"\"\"ì•ŒëŒ í”¼ë¡œë„ ì²´í¬\"\"\"\n",
        "        policy = self.alarm_policies[level]\n",
        "\n",
        "        # ìµœê·¼ ì•ŒëŒ í™•ì¸\n",
        "        recent_alarms = [\n",
        "            a for a in self.alarm_history\n",
        "            if a['equipment_id'] == equipment_id and\n",
        "            a['level'] == level and\n",
        "            (timestamp - a['timestamp']).total_seconds() < 3600\n",
        "        ]\n",
        "\n",
        "        # ë¹ˆë„ ì²´í¬\n",
        "        if len(recent_alarms) >= policy['max_frequency']:\n",
        "            return True\n",
        "\n",
        "        # ìµœì†Œ ê°„ê²© ì²´í¬\n",
        "        if recent_alarms:\n",
        "            last_alarm_time = recent_alarms[-1]['timestamp']\n",
        "            if (timestamp - last_alarm_time).total_seconds() < policy['min_interval']:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_alarm_statistics(self):\n",
        "        \"\"\"ì•ŒëŒ í†µê³„\"\"\"\n",
        "        if not self.alarm_history:\n",
        "            return {}\n",
        "\n",
        "        df = pd.DataFrame(self.alarm_history)\n",
        "\n",
        "        stats = {\n",
        "            'total_alarms': len(df),\n",
        "            'by_level': df['level'].value_counts().to_dict(),\n",
        "            'by_equipment': df['equipment_id'].value_counts().to_dict(),\n",
        "            'suppressed_count': 0  # ì‹¤ì œë¡œëŠ” ë³„ë„ë¡œ ì¶”ì  í•„ìš”\n",
        "        }\n",
        "\n",
        "        return stats\n",
        "\n",
        "# ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜\n",
        "class StreamingPipeline:\n",
        "    \"\"\"ì‹¤ì‹œê°„ ì˜ˆì§€ë³´ì „ íŒŒì´í”„ë¼ì¸\"\"\"\n",
        "\n",
        "    def __init__(self, model, alarm_system):\n",
        "        self.model = model  # ì´ìƒ íƒì§€ ëª¨ë¸\n",
        "        self.alarm_system = alarm_system\n",
        "        self.buffer = []  # ë°ì´í„° ë²„í¼\n",
        "        self.buffer_size = 100\n",
        "        self.results = []\n",
        "\n",
        "    def process_stream(self, data_point, timestamp, equipment_id):\n",
        "        \"\"\"ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬\"\"\"\n",
        "        # ë²„í¼ì— ì¶”ê°€\n",
        "        self.buffer.append(data_point)\n",
        "\n",
        "        # ë²„í¼ í¬ê¸° ìœ ì§€\n",
        "        if len(self.buffer) > self.buffer_size:\n",
        "            self.buffer.pop(0)\n",
        "\n",
        "        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ëª¨ì´ë©´ ë¶„ì„\n",
        "        if len(self.buffer) >= 50:\n",
        "            # íŠ¹ì§• ì¶”ì¶œ\n",
        "            features = self._extract_features(self.buffer[-50:])\n",
        "\n",
        "            # ì´ìƒ íƒì§€\n",
        "            anomaly_score = self._detect_anomaly(features)\n",
        "\n",
        "            # ì•ŒëŒ í‰ê°€\n",
        "            alarm = self.alarm_system.evaluate_alarm(\n",
        "                anomaly_score, timestamp, equipment_id\n",
        "            )\n",
        "\n",
        "            # ê²°ê³¼ ì €ì¥\n",
        "            result = {\n",
        "                'timestamp': timestamp,\n",
        "                'equipment_id': equipment_id,\n",
        "                'anomaly_score': anomaly_score,\n",
        "                'alarm': alarm\n",
        "            }\n",
        "\n",
        "            self.results.append(result)\n",
        "\n",
        "            return result\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_features(self, window_data):\n",
        "        \"\"\"ìœˆë„ìš° ë°ì´í„°ì—ì„œ íŠ¹ì§• ì¶”ì¶œ\"\"\"\n",
        "        # ê°„ë‹¨í•œ í†µê³„ íŠ¹ì§•\n",
        "        features = [\n",
        "            np.mean(window_data),\n",
        "            np.std(window_data),\n",
        "            np.max(window_data),\n",
        "            np.min(window_data),\n",
        "            kurtosis(window_data)\n",
        "        ]\n",
        "        return np.array(features)\n",
        "\n",
        "    def _detect_anomaly(self, features):\n",
        "        \"\"\"ì´ìƒ ì ìˆ˜ ê³„ì‚°\"\"\"\n",
        "        # ëª¨ë¸ ì˜ˆì¸¡ (ì‹œë®¬ë ˆì´ì…˜)\n",
        "        # ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©\n",
        "        anomaly_score = np.random.beta(2, 5)  # 0~1 ì‚¬ì´ ê°’\n",
        "\n",
        "        # íŠ¹ì§•ê°’ì´ ë¹„ì •ìƒì ì´ë©´ ì ìˆ˜ ì¦ê°€\n",
        "        if features[4] > 3:  # High kurtosis\n",
        "            anomaly_score = min(1.0, anomaly_score + 0.3)\n",
        "\n",
        "        return anomaly_score\n",
        "\n",
        "# ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜\n",
        "alarm_system = AlarmManagementSystem()\n",
        "pipeline = StreamingPipeline(if_model, alarm_system)\n",
        "\n",
        "# ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜\n",
        "print(\"ğŸ”„ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜...\\n\")\n",
        "\n",
        "stream_results = []\n",
        "equipment_ids = ['PUMP-001', 'MOTOR-002', 'BEARING-003']\n",
        "\n",
        "for i in range(200):\n",
        "    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±\n",
        "    timestamp = datetime.now() + timedelta(minutes=i)\n",
        "    equipment_id = np.random.choice(equipment_ids)\n",
        "\n",
        "    # ì‹œê°„ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì—´í™”\n",
        "    if i < 50:\n",
        "        data_point = np.random.normal(0, 1)\n",
        "    elif i < 100:\n",
        "        data_point = np.random.normal(0.5, 1.5)\n",
        "    elif i < 150:\n",
        "        data_point = np.random.normal(1, 2)\n",
        "    else:\n",
        "        data_point = np.random.normal(2, 3)\n",
        "\n",
        "    # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬\n",
        "    result = pipeline.process_stream(data_point, timestamp, equipment_id)\n",
        "\n",
        "    if result:\n",
        "        stream_results.append(result)\n",
        "\n",
        "        # ì•ŒëŒ ë°œìƒ ì‹œ ì¶œë ¥\n",
        "        if result['alarm']:\n",
        "            print(f\"âš ï¸ ALARM: {result['alarm']['level'].upper()}\")\n",
        "            print(f\"   Equipment: {result['alarm']['equipment_id']}\")\n",
        "            print(f\"   Score: {result['alarm']['score']:.3f}\")\n",
        "            print(f\"   Actions: {', '.join(result['alarm']['actions'])}\")\n",
        "            print()\n",
        "\n",
        "# ê²°ê³¼ ë¶„ì„\n",
        "stream_df = pd.DataFrame(stream_results)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Real-time Streaming Pipeline Results', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Anomaly scores over time\n",
        "axes[0, 0].plot(stream_df['timestamp'], stream_df['anomaly_score'])\n",
        "axes[0, 0].set_title('Anomaly Scores Over Time')\n",
        "axes[0, 0].set_xlabel('Time')\n",
        "axes[0, 0].set_ylabel('Anomaly Score')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Alarm triggers\n",
        "alarm_df = stream_df[stream_df['alarm'].notna()]\n",
        "if not alarm_df.empty:\n",
        "    alarm_levels = [a['level'] if a else None for a in alarm_df['alarm']]\n",
        "    level_colors = {'critical': 'red', 'high': 'orange', 'medium': 'yellow', 'low': 'green'}\n",
        "    colors = [level_colors.get(level, 'gray') for level in alarm_levels]\n",
        "\n",
        "    axes[0, 1].scatter(alarm_df['timestamp'], alarm_df['anomaly_score'],\n",
        "                      c=colors, s=50, alpha=0.7)\n",
        "    axes[0, 1].set_title('Alarm Triggers')\n",
        "    axes[0, 1].set_xlabel('Time')\n",
        "    axes[0, 1].set_ylabel('Anomaly Score')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Alarm statistics\n",
        "alarm_stats = alarm_system.get_alarm_statistics()\n",
        "if alarm_stats and 'by_level' in alarm_stats:\n",
        "    levels = list(alarm_stats['by_level'].keys())\n",
        "    counts = list(alarm_stats['by_level'].values())\n",
        "    axes[1, 0].bar(levels, counts, color=['red', 'orange', 'yellow', 'green'])\n",
        "    axes[1, 0].set_title('Alarms by Level')\n",
        "    axes[1, 0].set_xlabel('Alarm Level')\n",
        "    axes[1, 0].set_ylabel('Count')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Equipment-wise alarms\n",
        "if alarm_stats and 'by_equipment' in alarm_stats:\n",
        "    equipment = list(alarm_stats['by_equipment'].keys())\n",
        "    counts = list(alarm_stats['by_equipment'].values())\n",
        "    axes[1, 1].pie(counts, labels=equipment, autopct='%1.1f%%')\n",
        "    axes[1, 1].set_title('Alarms by Equipment')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ìµœì¢… í†µê³„\n",
        "print(\"\\nğŸ“Š ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ìµœì¢… í†µê³„:\")\n",
        "print(f\"   ì²˜ë¦¬ëœ ë°ì´í„° í¬ì¸íŠ¸: 200\")\n",
        "print(f\"   ë¶„ì„ëœ ìœˆë„ìš°: {len(stream_results)}\")\n",
        "if alarm_stats:\n",
        "    print(f\"   ë°œìƒí•œ ì•ŒëŒ: {alarm_stats.get('total_alarms', 0)}\")\n",
        "    print(f\"   ì•ŒëŒ ë¶„í¬: {alarm_stats.get('by_level', {})}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mrQ-LOihBQu"
      },
      "source": [
        "## 6. ì‹¤ìŠµ ê³¼ì œ ë° Best Practices\n",
        "\n",
        "### 6.1 ì˜ˆì§€ë³´ì „ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA_BRyMGhBQu"
      },
      "outputs": [],
      "source": [
        "# ì˜ˆì§€ë³´ì „ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° í…œí”Œë¦¿\n",
        "predictive_maintenance_checklist = {\n",
        "    'ë°ì´í„° ìˆ˜ì§‘': {\n",
        "        'tasks': [\n",
        "            'ì„¼ì„œ ì¢…ë¥˜ ë° ì‚¬ì–‘ ì •ì˜ (ì§„ë™, ì˜¨ë„, ì „ë¥˜, ìŒí–¥ ë“±)',\n",
        "            'ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ ì„¤ì • (Nyquist ì •ë¦¬ ê³ ë ¤)',\n",
        "            'ë°ì´í„° ì €ì¥ ì¸í”„ë¼ êµ¬ì¶•',\n",
        "            'ì •ìƒ/ê³ ì¥ ë¼ë²¨ ë°ì´í„° í™•ë³´'\n",
        "        ],\n",
        "        'considerations': [\n",
        "            'ì§„ë™ ì„¼ì„œ: ìµœì†Œ 10kHz ìƒ˜í”Œë§',\n",
        "            'ì˜¨ë„ ì„¼ì„œ: 1Hz ì´ìƒ',\n",
        "            'ìµœì†Œ 3ê°œì›” ì´ìƒ ë°ì´í„° ìˆ˜ì§‘ ê¶Œì¥'\n",
        "        ]\n",
        "    },\n",
        "    'íŠ¹ì§• ì¶”ì¶œ': {\n",
        "        'time_domain': [\n",
        "            'RMS (Root Mean Square)',\n",
        "            'Kurtosis (ì¶©ê²© ì‹ í˜¸ ê°ì§€)',\n",
        "            'Crest Factor',\n",
        "            'Peak-to-Peak'\n",
        "        ],\n",
        "        'frequency_domain': [\n",
        "            'FFT Spectrum',\n",
        "            'Envelope Analysis',\n",
        "            'Order Analysis',\n",
        "            'Spectral Entropy'\n",
        "        ],\n",
        "        'time_frequency': [\n",
        "            'Wavelet Transform',\n",
        "            'STFT (Short-Time Fourier Transform)',\n",
        "            'Hilbert-Huang Transform'\n",
        "        ]\n",
        "    },\n",
        "    'ëª¨ë¸ ì„ íƒ': {\n",
        "        'unsupervised': [\n",
        "            'Isolation Forest: ë¹ ë¥´ê³  íš¨ê³¼ì ',\n",
        "            'One-Class SVM: ë¹„ì„ í˜• íŒ¨í„´',\n",
        "            'Autoencoder: ë³µì¡í•œ íŒ¨í„´'\n",
        "        ],\n",
        "        'supervised': [\n",
        "            'Random Forest: í•´ì„ ê°€ëŠ¥',\n",
        "            'XGBoost: ë†’ì€ ì„±ëŠ¥',\n",
        "            'LSTM: ì‹œê³„ì—´ íŒ¨í„´'\n",
        "        ]\n",
        "    },\n",
        "    'ë°°í¬ ë° ìš´ì˜': {\n",
        "        'deployment': [\n",
        "            'Edge Computing ê³ ë ¤',\n",
        "            'API ì„œë²„ êµ¬ì¶•',\n",
        "            'ì‹¤ì‹œê°„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸',\n",
        "            'ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ'\n",
        "        ],\n",
        "        'maintenance': [\n",
        "            'ëª¨ë¸ ì¬í•™ìŠµ ì£¼ê¸° ì„¤ì •',\n",
        "            'ë“œë¦¬í”„íŠ¸ ê°ì§€',\n",
        "            'ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§',\n",
        "            'A/B í…ŒìŠ¤íŠ¸'\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# RUL (Remaining Useful Life) ì˜ˆì¸¡ í…œí”Œë¦¿\n",
        "rul_template = {\n",
        "    'equipment_info': {\n",
        "        'equipment_id': '',\n",
        "        'type': '',  # pump, motor, bearing, etc.\n",
        "        'installation_date': '',\n",
        "        'operating_hours': 0,\n",
        "        'maintenance_history': []\n",
        "    },\n",
        "    'sensor_config': {\n",
        "        'vibration': {'sampling_rate': 10000, 'channels': 3},\n",
        "        'temperature': {'sampling_rate': 1, 'channels': 1},\n",
        "        'current': {'sampling_rate': 100, 'channels': 3}\n",
        "    },\n",
        "    'model_config': {\n",
        "        'type': 'LSTM',\n",
        "        'input_features': [],\n",
        "        'prediction_horizon': 30,  # days\n",
        "        'confidence_interval': 0.95\n",
        "    },\n",
        "    'alert_config': {\n",
        "        'rul_thresholds': {\n",
        "            'critical': 7,   # days\n",
        "            'warning': 30,   # days\n",
        "            'notice': 90     # days\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¶œë ¥\n",
        "print(\"âœ… ì˜ˆì§€ë³´ì „ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸\\n\")\n",
        "for category, items in predictive_maintenance_checklist.items():\n",
        "    print(f\"ğŸ“Œ {category}:\")\n",
        "    for key, values in items.items():\n",
        "        print(f\"\\n  [{key.upper()}]\")\n",
        "        for value in values:\n",
        "            print(f\"    â–¡ {value}\")\n",
        "    print()\n",
        "\n",
        "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
        "import json\n",
        "\n",
        "with open('predictive_maintenance_template.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump({\n",
        "        'checklist': predictive_maintenance_checklist,\n",
        "        'rul_template': rul_template\n",
        "    }, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"\\nğŸ’¾ í…œí”Œë¦¿ì´ 'predictive_maintenance_template.json'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mdyxw6ohBQu"
      },
      "source": [
        "## 7. Session 3 í•µì‹¬ ìš”ì•½\n",
        "\n",
        "### ì´ë²ˆ ì„¸ì…˜ì—ì„œ í•™ìŠµí•œ ë‚´ìš©:\n",
        "\n",
        "1. **ì‹œê³„ì—´ íŠ¹ì§• ì¶”ì¶œ**\n",
        "   - ì‹œê°„ ë„ë©”ì¸: RMS, Kurtosis, Crest Factor\n",
        "   - ì£¼íŒŒìˆ˜ ë„ë©”ì¸: FFT, Spectral Features\n",
        "   - Envelope Analysis for bearing faults\n",
        "\n",
        "2. **ì´ìƒ íƒì§€ ëª¨ë¸ ë¹„êµ**\n",
        "   - Isolation Forest: ë¹ ë¥´ê³  íš¨ìœ¨ì \n",
        "   - One-Class SVM: ë¹„ì„ í˜• ê²½ê³„\n",
        "   - LSTM Autoencoder: ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ\n",
        "\n",
        "3. **ì•ŒëŒ ê´€ë¦¬ ì‹œìŠ¤í…œ**\n",
        "   - ì•ŒëŒ í”¼ë¡œë„ ë°©ì§€ ì „ëµ\n",
        "   - ë‹¤ë‹¨ê³„ ì•ŒëŒ ì •ì±…\n",
        "   - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬\n",
        "\n",
        "4. **ì‹¤ì „ ê³ ë ¤ì‚¬í•­**\n",
        "   - ì„¼ì„œ ì‚¬ì–‘ ë° ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜\n",
        "   - Edge vs Cloud ì²˜ë¦¬\n",
        "   - ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ê´€ë¦¬\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "1. **ë°ì´í„° í’ˆì§ˆ**\n",
        "   - ì¶©ë¶„í•œ ì •ìƒ ë°ì´í„° í™•ë³´ (ìµœì†Œ 3ê°œì›”)\n",
        "   - ë‹¤ì–‘í•œ ìš´ì „ ì¡°ê±´ í¬í•¨\n",
        "   - ì„¼ì„œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì •ê¸° ìˆ˜í–‰\n",
        "\n",
        "2. **ëª¨ë¸ ì„ íƒ**\n",
        "   - ì´ˆê¸°: ë‹¨ìˆœ í†µê³„ ê¸°ë°˜ ë°©ë²•\n",
        "   - ì¤‘ê¸°: Isolation Forest, OCSVM\n",
        "   - ê³ ë„í™”: Deep Learning (ë°ì´í„° ì¶©ë¶„ ì‹œ)\n",
        "\n",
        "3. **ìš´ì˜ ìµœì í™”**\n",
        "   - ì ì§„ì  ì„ê³„ê°’ ì¡°ì •\n",
        "   - í˜„ì¥ í”¼ë“œë°± ë°˜ì˜\n",
        "   - ì •ê¸°ì  ì¬í•™ìŠµ\n",
        "\n",
        "### ROI ê·¹ëŒ€í™” ì „ëµ:\n",
        "- Critical Equipment ìš°ì„  ì ìš©\n",
        "- ê³ ì¥ ë¹„ìš©ì´ ë†’ì€ ì„¤ë¹„ ì§‘ì¤‘\n",
        "- ë‹¨ê³„ì  í™•ëŒ€ (Pilot â†’ Scale-up)\n",
        "- ìš´ì˜ ì¸ë ¥ êµìœ¡ ë³‘í–‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUobuz-NhBQu"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ¯ ì œì¡° AI 6ì‹œê°„ ê³¼ì • ì™„ë£Œ!\\n\")\n",
        "print(\"í•™ìŠµí•œ ë‚´ìš©:\")\n",
        "print(\"âœ… Session 1: ì œì¡° ë°ì´í„° ì•„í‚¤í…ì²˜ & ê³¼ì œ ì •ì˜\")\n",
        "print(\"âœ… Session 2: ë¹„ì „ ë¶ˆëŸ‰ê²€ì‚¬ ê³ ë„í™”\")\n",
        "print(\"âœ… Session 3: ì˜ˆì§€ë³´ì „(ì‹œê³„ì—´) ì‹¬í™”\\n\")\n",
        "print(\"ğŸ’¡ ì‹¤ë¬´ ì ìš© íŒ:\")\n",
        "print(\"1. Small Start, Quick Win ì „ëµ\")\n",
        "print(\"2. í˜„ì¥ ì‘ì—…ìì™€ ê¸´ë°€í•œ í˜‘ì—…\")\n",
        "print(\"3. ì§€ì†ì  ëª¨ë‹ˆí„°ë§ê³¼ ê°œì„ \")\n",
        "print(\"4. ROI ì¤‘ì‹¬ ì˜ì‚¬ê²°ì •\\n\")\n",
        "print(\"ğŸ“§ ì¶”ê°€ ë¬¸ì˜: imguru.co.kr\")\n",
        "print(\"\\nê°ì‚¬í•©ë‹ˆë‹¤! ğŸ™\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}